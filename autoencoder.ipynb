{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.science.org/doi/10.1126/science.1127647\n",
    "\n",
    "https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/conv_autoencoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from lib.modules import (\n",
    "    evaluate_loop, \n",
    "    pad_for_windowing,\n",
    "    window_session,\n",
    "    optimization_loop,\n",
    "    optimization_loop_xonly\n",
    ")\n",
    "from lib.models import ResAutoEncoder\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.datasets import AccRawDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processesing\n",
    "1. Get list of N raw recording directories (from delta app, no labels)\n",
    "2. Read all raw data into N DataFrames of lengths l. Reset their timestamp to be seconds from the start. Print the length of each session\n",
    "3. Read and pad N recordings into N tensors of sizes (l+100 x 3). Then concatonate all recordings into a single tensor of size (L x 3)\n",
    "4. Cut off end of tensor so that C|L. Split the single tensor into chunks of C=1.8e6 samples (5 hours) to get a tensor of size (L/C x C x 3).\n",
    "5. Pad each chunk to get a tensor of size (L/C x C+100 x 3). This is so that no windows overlap two chunks\n",
    "6. Take 75% of chunks to be training chunks, and take the other 25% to be test chunks\n",
    "7. Seperately flatten the first two dims of train and test chunks to get tensors of size (L1 x 3) and (L2 x 3)\n",
    "8. Create an AccRawDataset for train and test chunks. This dataset will create windows dynamically to return tensors of size (303). It will have size L-101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINSIZE = 101\n",
    "DEVICE = 'cuda:0'\n",
    "RAW_DIR = Path('/home/musa/datasets/eating_raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-07_20_24_32\n",
      "2023-10-26_15_32_20\n",
      "11-07_17_43_30\n",
      "11-08_08_27_30\n",
      "11-08_07_17_47\n",
      "11-10_08_54_24\n",
      "2023-11-01_15_49_48\n",
      "11-07_12_58_43\n",
      "11-01_20_34_28\n",
      "10-27_00_21_25\n",
      "11-07_17_29_01\n",
      "11-01_20_54_52\n",
      "11-07_15_03_24\n",
      "10-27_09_45_42\n",
      "11-02_19_28_19\n",
      "10-28_13_18_42\n",
      "10-27_00_20_15\n"
     ]
    }
   ],
   "source": [
    "recordings = []\n",
    "for rec_dir in RAW_DIR.iterdir():\n",
    "    recordings.append(rec_dir)\n",
    "    print(rec_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Date: 11-07_20_24_32, nSamples: 30117, Time Elapsed: 0:04:49.392967, Time Recorded: 0:05:01.170000\n",
      "Index: 1, Date: 2023-10-26_15_32_20, nSamples: 2961601, Time Elapsed: 7:53:59.529118, Time Recorded: 8:13:36.010000\n",
      "Index: 2, Date: 11-07_17_43_30, nSamples: 1005447, Time Elapsed: 2:41:00.547341, Time Recorded: 2:47:34.470000\n",
      "Index: 3, Date: 11-08_08_27_30, nSamples: 5125043, Time Elapsed: 1 day, 6:08:45.212074, Time Recorded: 14:14:10.430000\n",
      "Index: 4, Date: 11-08_07_17_47, nSamples: 338215, Time Elapsed: 0:54:09.386096, Time Recorded: 0:56:22.150000\n",
      "Index: 5, Date: 11-10_08_54_24, nSamples: 1370732, Time Elapsed: 5:06:32.989927, Time Recorded: 3:48:27.320000\n",
      "Index: 6, Date: 2023-11-01_15_49_48, nSamples: 2910132, Time Elapsed: 7:45:46.365347, Time Recorded: 8:05:01.320000\n",
      "Index: 7, Date: 11-07_12_58_43, nSamples: 776721, Time Elapsed: 2:04:21.712932, Time Recorded: 2:09:27.210000\n",
      "Index: 8, Date: 11-01_20_34_28, nSamples: 127133, Time Elapsed: 0:20:21.474008, Time Recorded: 0:21:11.330000\n",
      "Index: 9, Date: 10-27_00_21_25, nSamples: 260457, Time Elapsed: 0:51:35.645162, Time Recorded: 0:43:24.570000\n",
      "Index: 10, Date: 11-07_17_29_01, nSamples: 90052, Time Elapsed: 0:14:25.199006, Time Recorded: 0:15:00.520000\n",
      "Index: 11, Date: 11-01_20_54_52, nSamples: 4080424, Time Elapsed: 22:11:22.872910, Time Recorded: 11:20:04.240000\n",
      "Index: 12, Date: 11-07_15_03_24, nSamples: 696054, Time Elapsed: 1:51:27.930080, Time Recorded: 1:56:00.540000\n",
      "Index: 13, Date: 10-27_09_45_42, nSamples: 2854125, Time Elapsed: 7:37:03.002257, Time Recorded: 7:55:41.250000\n",
      "Index: 14, Date: 11-02_19_28_19, nSamples: 9274120, Time Elapsed: 2 days, 14:45:07.943559, Time Recorded: 1 day, 1:45:41.200000\n",
      "Index: 15, Date: 10-28_13_18_42, nSamples: 19536627, Time Elapsed: 4 days, 5:58:05.118965, Time Recorded: 2 days, 6:16:06.270000\n",
      "Index: 16, Date: 10-27_00_20_15, nSamples: 6919, Time Elapsed: 0:01:06.511446, Time Recorded: 0:01:09.190000\n"
     ]
    }
   ],
   "source": [
    "accelerations = []\n",
    "for session_dir in recordings:\n",
    "    accel_file = session_dir / f'acceleration-{session_dir.name}.csv'\n",
    "    if not accel_file.is_file():\n",
    "        accel_file = session_dir / f'acceleration.csv'\n",
    "\n",
    "    acceleration = pd.read_csv(accel_file,skiprows=1).rename({'x': 'x_acc', 'y': 'y_acc', 'z': 'z_acc'}, axis=1)\n",
    "    acceleration = acceleration.dropna()\n",
    "\n",
    "    acceleration_start_time_seconds = float(pd.read_csv(session_dir / accel_file, nrows=1,header=None).iloc[0,0].split()[-1])/1000\n",
    "    acceleration.timestamp = ((acceleration.timestamp - acceleration.timestamp[0])*1e-9)+acceleration_start_time_seconds # get timestamp in seconds\n",
    "\n",
    "    accelerations.append(acceleration)\n",
    "\n",
    "    print(f'Index: {len(accelerations)-1}, Date: {session_dir.name}, nSamples: {len(acceleration)}, Time Elapsed: {timedelta(seconds=acceleration.timestamp.iloc[-1] - acceleration.timestamp.iloc[0])}, Time Recorded: {timedelta(seconds=len(acceleration) / 100)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for acc in accelerations:\n",
    "    accs.append(pad_for_windowing(torch.Tensor(acc[['x_acc','y_acc','z_acc']].values), WINSIZE))\n",
    "\n",
    "chunk_len = 5 * 60 * 60 * 100 # = 1,800,000 samples ie. 5 hours of recording\n",
    "all_acc = torch.cat(accs, axis=0)\n",
    "all_acc = all_acc[:len(all_acc) - len(all_acc) % chunk_len] # cut off very last part\n",
    "all_acc = all_acc.view(-1, chunk_len, 3)\n",
    "\n",
    "np.random.seed(10)\n",
    "def proc(x):\n",
    "    x = pad_for_windowing(x, WINSIZE) # pad second dimension\n",
    "    x = x.flatten(end_dim=1)\n",
    "    return x\n",
    "acctr, accte = map(proc, train_test_split(all_acc, test_size=0.25))\n",
    "\n",
    "Xtr = AccRawDataset(acctr, WINSIZE)\n",
    "Xte = AccRawDataset(accte, WINSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_acc.flatten(end_dim=1)) / 100 / 60 / 60)\n",
    "print(len(acctr) / 100 / 60 / 60)\n",
    "print(len(accte) / 100 / 60 / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(Xtr, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(Xte, batch_size=64)\n",
    "\n",
    "torch.save(trainloader, 'pytorch_datasets/trainloader_11-25-23.pt')\n",
    "torch.save(testloader, 'pytorch_datasets/testloader_11-25-23.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_amt = 20\n",
    "i = 5\n",
    "fig = px.line(accelerations[i][::dim_amt], x=accelerations[i].index[::dim_amt], y=['x_acc','y_acc','z_acc'])\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.load('pytorch_datasets/trainloader_11-25-23.pt')\n",
    "testloader = torch.load('pytorch_datasets/testloader_11-25-23.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResAutoEncoder(WINSIZE, 3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_loop_xonly(model, trainloader, testloader, criterion, optimizer, 5, DEVICE, Path('dev/test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'dev/autoencoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.load('pytorch_datasets/trainloader_11-16-23.pt')\n",
    "testloader = torch.load('pytorch_datasets/testloader_11-16-23.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoderImproved(winsize=WINSIZE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('dev/autoencoder4_conv-impr/best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate a signal with trained model\n",
    "\n",
    "Xtrue = []\n",
    "Xpred = []\n",
    "\n",
    "for i,X in tqdm(enumerate(testloader)):\n",
    "    if i > 2000:\n",
    "        break\n",
    "\n",
    "    X = X.to(DEVICE)\n",
    "    logits = model(X)\n",
    "    Xtrue.append(X.detach().cpu().view(-1,3,101)[:,:,50])\n",
    "    Xpred.append(logits.detach().cpu().view(-1,3,101)[:,:,50])\n",
    "\n",
    "Xtrue = torch.cat(Xtrue).T\n",
    "Xpred = torch.cat(Xpred).T\n",
    "\n",
    "acceleration = pd.DataFrame()\n",
    "acceleration['x_acc'] = Xtrue[0]\n",
    "acceleration['y_acc'] = Xtrue[1]\n",
    "acceleration['z_acc'] = Xtrue[2]\n",
    "acceleration['x_pred'] = Xpred[0]\n",
    "acceleration['y_pred'] = Xpred[1]\n",
    "acceleration['z_pred'] = Xpred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_amt = 1\n",
    "fig = px.line(acceleration[::dim_amt], x=acceleration.index[::dim_amt], y=['x_acc','y_acc','z_acc', 'x_pred', 'y_pred', 'z_pred'])\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.modules import read_and_window_nursing_session, read_nursing_session\n",
    "\n",
    "i = 58\n",
    "X,y = read_and_window_nursing_session(i, WINSIZE, Path('/home/musa/datasets/nursingv1'), Path('/home/musa/datasets/eating_labels/'))\n",
    "testloader = DataLoader(TensorDataset(X), batch_size=64)\n",
    "acceleration = read_nursing_session(i, Path('/home/musa/datasets/nursingv1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate a signal with trained model\n",
    "\n",
    "Xpred = []\n",
    "for X in tqdm(testloader):\n",
    "    X = X[0].to(DEVICE)\n",
    "    logits = model(X)\n",
    "    Xpred.append(logits)\n",
    "\n",
    "Xpred = torch.cat(Xpred)\n",
    "Xpred = Xpred.view(-1,3,101)[:,:,50].T # unwindow\n",
    "\n",
    "acceleration['x_pred'] = Xpred[0].cpu().detach()\n",
    "acceleration['y_pred'] = Xpred[1].cpu().detach()\n",
    "acceleration['z_pred'] = Xpred[2].cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_amt = 5\n",
    "fig = px.line(acceleration[::dim_amt], x=acceleration.index[::dim_amt], y=['x_acc','y_acc','z_acc', 'x_pred', 'y_pred', 'z_pred'])\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = 0\n",
    "for X in trainloader:\n",
    "    X = X[0][0].view(3,101).to(DEVICE)\n",
    "    logits = model.encoder[0](X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logits[2].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict().keys())\n",
    "plt.plot(model.state_dict()['encoder.0.weight'][3][0].cpu().detach())\n",
    "plt.plot(model.state_dict()['encoder.0.weight'][3][1].cpu().detach())\n",
    "plt.plot(model.state_dict()['encoder.0.weight'][3][2].cpu().detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "07a7e9baaa56ad5291b67d7a98d93b7d3a1f6016f598f84e49730f5552732cf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
