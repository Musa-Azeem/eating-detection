{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from lib.config import *\n",
    "from lib.data.dataloading import load_raw\n",
    "from lib.modules import optimization_loop_xonly\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all available sessions\n",
      "Using Directories: ['11-07_20_24_32', '2023-10-26_15_32_20', '11-07_17_43_30', '11-08_08_27_30', '11-08_07_17_47', '11-10_08_54_24', '2023-11-11_17_50_20', '2023-11-01_15_47_52', '2023-11-01_15_49_48', '11-07_12_58_43', '2023-11-10_13_11_41', '2023-11-02_13_55_22', '11-01_20_34_28', '10-27_00_21_25', '11-07_17_29_01', '11-01_20_54_52', '11-07_15_03_24', '10-27_09_45_42', '11-02_19_28_19', '10-28_13_18_42', '10-27_00_20_15']\n",
      "Index: 0, Date: 11-07_20_24_32, nSamples: 30117, Time Elapsed: 0:04:49.392967, Time Recorded: 0:05:01.170000\n",
      "Index: 1, Date: 2023-10-26_15_32_20, nSamples: 2961601, Time Elapsed: 7:53:59.529118, Time Recorded: 8:13:36.010000\n",
      "Index: 2, Date: 11-07_17_43_30, nSamples: 1005447, Time Elapsed: 2:41:00.547341, Time Recorded: 2:47:34.470000\n",
      "Index: 3, Date: 11-08_08_27_30, nSamples: 5125043, Time Elapsed: 1 day, 6:08:45.212074, Time Recorded: 14:14:10.430000\n",
      "Index: 4, Date: 11-08_07_17_47, nSamples: 338215, Time Elapsed: 0:54:09.386096, Time Recorded: 0:56:22.150000\n",
      "Index: 5, Date: 11-10_08_54_24, nSamples: 1370732, Time Elapsed: 5:06:32.989927, Time Recorded: 3:48:27.320000\n",
      "Index: 6, Date: 2023-11-11_17_50_20, nSamples: 624412, Time Elapsed: 2 days, 21:18:20.815201, Time Recorded: 1:44:04.120000\n",
      "Index: 7, Date: 2023-11-01_15_47_52, nSamples: 1726229, Time Elapsed: 4:35:26.371657, Time Recorded: 4:47:42.290000\n",
      "Index: 8, Date: 2023-11-01_15_49_48, nSamples: 2910132, Time Elapsed: 7:45:46.365347, Time Recorded: 8:05:01.320000\n",
      "Index: 9, Date: 11-07_12_58_43, nSamples: 776721, Time Elapsed: 2:04:21.712932, Time Recorded: 2:09:27.210000\n",
      "Index: 10, Date: 2023-11-10_13_11_41, nSamples: 3139089, Time Elapsed: 15:52:51.098452, Time Recorded: 8:43:10.890000\n",
      "Index: 11, Date: 2023-11-02_13_55_22, nSamples: 2499818, Time Elapsed: 13:04:33.095864, Time Recorded: 6:56:38.180000\n",
      "Index: 12, Date: 11-01_20_34_28, nSamples: 127133, Time Elapsed: 0:20:21.474008, Time Recorded: 0:21:11.330000\n",
      "Index: 13, Date: 10-27_00_21_25, nSamples: 260457, Time Elapsed: 0:51:35.645162, Time Recorded: 0:43:24.570000\n",
      "Index: 14, Date: 11-07_17_29_01, nSamples: 90052, Time Elapsed: 0:14:25.199006, Time Recorded: 0:15:00.520000\n",
      "Index: 15, Date: 11-01_20_54_52, nSamples: 4080424, Time Elapsed: 22:11:22.872910, Time Recorded: 11:20:04.240000\n",
      "Index: 16, Date: 11-07_15_03_24, nSamples: 696054, Time Elapsed: 1:51:27.930080, Time Recorded: 1:56:00.540000\n",
      "Index: 17, Date: 10-27_09_45_42, nSamples: 2854125, Time Elapsed: 7:37:03.002257, Time Recorded: 7:55:41.250000\n",
      "Index: 18, Date: 11-02_19_28_19, nSamples: 9274120, Time Elapsed: 2 days, 14:45:07.943559, Time Recorded: 1 day, 1:45:41.200000\n",
      "Index: 19, Date: 10-28_13_18_42, nSamples: 19536627, Time Elapsed: 4 days, 5:58:05.118965, Time Recorded: 2 days, 6:16:06.270000\n",
      "Index: 20, Date: 10-27_00_20_15, nSamples: 6919, Time Elapsed: 0:01:06.511446, Time Recorded: 0:01:09.190000\n",
      "Created 16515 chunks of length 3600 samples each\n",
      "Randomly selected 100 chunks\n",
      "Total train length: 0:38:20 (230000 Samples)\n",
      "Total test length: 0:38:20 (230000 Samples)\n"
     ]
    }
   ],
   "source": [
    "WINSIZE = 1001\n",
    "trainloader, testloader = load_raw(\n",
    "    RAW_DIR,\n",
    "    WINSIZE,\n",
    "    n_hours=1,\n",
    "    # sessions=['2023-11-02_13_55_22'],\n",
    "    test_size=0.5,\n",
    "    batch_size=128,\n",
    "    shuffle_test=True,\n",
    "    chunk_len_hrs=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockMAE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, seq_len, relu=True, p_dropout=None):\n",
    "        super().__init__()\n",
    "        self.use_relu = relu\n",
    "        self.c = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.LayerNorm((out_channels, seq_len)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.LayerNorm((out_channels, seq_len)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.LayerNorm((out_channels, seq_len)),\n",
    "        )\n",
    "        if self.use_relu:\n",
    "            self.c.add_module('relu', nn.ReLU())\n",
    "        if p_dropout is not None:\n",
    "            self.c.add_module('dropout', nn.Dropout(p=p_dropout))\n",
    "\n",
    "        self.identity = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.LayerNorm((out_channels, seq_len))\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c(x) + self.identity(x)\n",
    "        return self.relu(x) if self.use_relu else x\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.dims)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term =  torch.pow(10000.0, torch.arange(0, d_model, 2) / d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        return x\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, winsize, in_channels, mask_chunk_size=11, enc_dims=(8,16,32,64,96,128), d_model=192, maskpct=0.75):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.winsize = winsize\n",
    "        self.enc_dims = enc_dims\n",
    "        self.d_model = d_model\n",
    "        self.mask_chunk_size = mask_chunk_size\n",
    "        self.maskpct = maskpct\n",
    "        p_dropout = 0.01\n",
    "        \n",
    "        self.e = nn.Sequential(\n",
    "            ResBlockMAE(in_channels, enc_dims[0], 5, 'same', winsize),\n",
    "            *[ResBlockMAE(self.enc_dims[i], self.enc_dims[i+1], 3, 'same', winsize, p_dropout=p_dropout) for i in range(len(self.enc_dims)-1)]\n",
    "        )\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            nn.Conv1d(enc_dims[-1], d_model, 1),\n",
    "            Permute(0,2,1),\n",
    "            PositionalEncoding(d_model, seq_len=winsize),\n",
    "            nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model, 1, 2048, 0.1, batch_first=True), \n",
    "                1,\n",
    "                enable_nested_tensor=False\n",
    "            ),\n",
    "            Permute(0,2,1),\n",
    "        )\n",
    "        self.d = nn.Sequential(\n",
    "            ResBlockMAE(d_model, enc_dims[-1], 3, 'same', winsize),\n",
    "            nn.Conv1d(enc_dims[-1], in_channels, 3, padding='same'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, self.winsize)\n",
    "        x = self.e(x)\n",
    "        x = self.mask(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.d(x)\n",
    "        return x.flatten(start_dim=1)\n",
    "    \n",
    "    def mask(self, x):\n",
    "        # Mask: split X into chunks and randomly set maskpct% of chunks \n",
    "        # (all 64 dims) to values from a normal distribution\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2]//self.mask_chunk_size, -1).clone()\n",
    "        mask = torch.rand(x.shape[0], 1, x.shape[2]) < self.maskpct # maskpct% of values are True\n",
    "        mask = mask.expand(-1, x.shape[1], -1)                      # expand to all 64 dims\n",
    "        x[mask] = torch.randn(x.shape, device=x.device)[mask]       # set masked chunks to random values\n",
    "        x = x.flatten(start_dim=2)                                  # get rid of chunk dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4155075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda:1'\n",
    "model = TransformerEncoder(WINSIZE, 3, enc_dims=(32,64,128), d_model=192, maskpct=0.25).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.MSELoss()\n",
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": Epoch 1: Train Loss: 0.14516: Dev Loss: 0.12016:  10%|█         | 2/20 [13:08<1:58:13, 394.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimization_loop_xonly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev/8_transformer/dev_mask25_2hrs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_transformer_mask25_2hrs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eating/eating-detection/lib/modules.py:398\u001b[0m, in \u001b[0;36moptimization_loop_xonly\u001b[0;34m(model, trainloader, devloader, criterion, optimizer, epochs, device, patience, min_delta, outdir, label, writer)\u001b[0m\n\u001b[1;32m    395\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Train Loop\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m train_lossi \u001b[38;5;241m=\u001b[39m \u001b[43minner_train_loop_xonly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(train_lossi) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader))            \n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Dev Loop\u001b[39;00m\n",
      "File \u001b[0;32m~/eating/eating-detection/lib/modules.py:471\u001b[0m, in \u001b[0;36minner_train_loop_xonly\u001b[0;34m(model, trainloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    470\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 471\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    474\u001b[0m lossi\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/eating/eating-detection/env/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eating/eating-detection/env/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimization_loop_xonly(\n",
    "    model,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs=20,\n",
    "    device=DEVICE,\n",
    "    outdir='dev/8_mae/dev_mask25_2hrs',\n",
    "    writer=f'runs/{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}_delta_mask25_2hrs'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
